{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import InputLayer\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import *\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I initialise the file paths for the images and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'D:\\Admin\\Documents\\Year_4\\AMLS\\Assessment\\dataset_AMLS_20-21\\celeba\\img'\n",
    "labels_path = 'D:\\Admin\\Documents\\Year_4\\AMLS\\Assessment\\dataset_AMLS_20-21\\celeba\\labels.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I import the images and labels into a training data frame using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "dft = pd.read_csv(labels_path, skiprows=lambda x: x in range(1,5000,5))\n",
    "dft[\"smiling\"] = dft[\"smiling\"].replace(to_replace=[-1], value=['Frown'])\n",
    "dft[\"smiling\"] = dft[\"smiling\"].replace(to_replace=[1], value=['Smile'])\n",
    "dft[\"gender\"] = dft[\"gender\"].replace(to_replace=[-1], value=['Female'])\n",
    "dft[\"gender\"] = dft[\"gender\"].replace(to_replace=[1], value=['Male'])\n",
    "one_hot_s = pd.get_dummies(dft[\"smiling\"])\n",
    "one_hot_g = pd.get_dummies(dft[\"gender\"])\n",
    "dft = dft.drop(columns=[\"gender\", \"smiling\"])\n",
    "dft = dft.join(one_hot_g)\n",
    "dft = dft.join(one_hot_s)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=dft, \n",
    "    directory=file_path,\n",
    "    x_col=\"img_name\", \n",
    "    y_col=[\"Frown\", \"Smile\"], \n",
    "    class_mode=\"raw\", \n",
    "    target_size=(55,45), \n",
    "    batch_size=100, \n",
    "    color_mode='grayscale', \n",
    "    interpolation='bicubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I do the same thing but for a testing data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 999 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "dfv = pd.read_csv(labels_path, skiprows=lambda x: x not in range(0,5000,5))\n",
    "dfv[\"smiling\"] = dfv[\"smiling\"].replace(to_replace=[-1], value=['Frown'])\n",
    "dfv[\"smiling\"] = dfv[\"smiling\"].replace(to_replace=[1], value=['Smile'])\n",
    "dfv[\"gender\"] = dfv[\"gender\"].replace(to_replace=[-1], value=['Female'])\n",
    "dfv[\"gender\"] = dfv[\"gender\"].replace(to_replace=[1], value=['Male'])\n",
    "one_hot_s = pd.get_dummies(dfv[\"smiling\"])\n",
    "one_hot_g = pd.get_dummies(dfv[\"gender\"])\n",
    "dfv = dfv.drop(columns=[\"gender\", \"smiling\"])\n",
    "dfv = dfv.join(one_hot_g)\n",
    "dfv = dfv.join(one_hot_s)\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=dfv, \n",
    "    directory=file_path,\n",
    "    x_col=\"img_name\", \n",
    "    y_col=[\"Frown\", \"Smile\"], \n",
    "    class_mode=\"raw\", \n",
    "    target_size=(55,45), \n",
    "    batch_size=100, \n",
    "    color_mode='grayscale', \n",
    "    interpolation='bicubic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code generates batches of data to feed into the neural network architecture below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(55,45,1)))\n",
    "model.add(Conv2D(filters=100, kernel_size=(3,3), strides=2, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=200, kernel_size=(2,2), strides=2, activation=\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Conv2D(filters=400,kernel_size=(2,2),activation=\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code just compiles the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = train_generator.n//train_generator.batch_size\n",
    "test_steps = test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code just trains the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "40/40 [==============================] - 20s 502ms/step - loss: 0.7734 - accuracy: 0.6185 - val_loss: 0.6877 - val_accuracy: 0.5778\n",
      "Epoch 2/25\n",
      "40/40 [==============================] - 22s 542ms/step - loss: 0.5186 - accuracy: 0.7513 - val_loss: 0.6864 - val_accuracy: 0.5239\n",
      "Epoch 3/25\n",
      "40/40 [==============================] - 20s 510ms/step - loss: 0.4412 - accuracy: 0.7897 - val_loss: 0.7027 - val_accuracy: 0.5017\n",
      "Epoch 4/25\n",
      "40/40 [==============================] - 19s 487ms/step - loss: 0.3723 - accuracy: 0.8303 - val_loss: 0.7791 - val_accuracy: 0.5195\n",
      "Epoch 5/25\n",
      "40/40 [==============================] - 20s 493ms/step - loss: 0.3276 - accuracy: 0.8570 - val_loss: 0.8124 - val_accuracy: 0.5139\n",
      "Epoch 6/25\n",
      "40/40 [==============================] - 19s 483ms/step - loss: 0.2687 - accuracy: 0.8905 - val_loss: 0.7633 - val_accuracy: 0.5161\n",
      "Epoch 7/25\n",
      "40/40 [==============================] - 20s 488ms/step - loss: 0.2261 - accuracy: 0.9053 - val_loss: 1.0662 - val_accuracy: 0.5095\n",
      "Epoch 8/25\n",
      "40/40 [==============================] - 19s 484ms/step - loss: 0.1709 - accuracy: 0.9348 - val_loss: 1.0594 - val_accuracy: 0.5150\n",
      "Epoch 9/25\n",
      "40/40 [==============================] - 19s 469ms/step - loss: 0.1412 - accuracy: 0.9482 - val_loss: 0.8294 - val_accuracy: 0.5673\n",
      "Epoch 10/25\n",
      "40/40 [==============================] - 19s 470ms/step - loss: 0.0993 - accuracy: 0.9675 - val_loss: 0.8055 - val_accuracy: 0.6329\n",
      "Epoch 11/25\n",
      "40/40 [==============================] - 11s 280ms/step - loss: 0.0701 - accuracy: 0.9775 - val_loss: 0.5646 - val_accuracy: 0.7289\n",
      "Epoch 12/25\n",
      "40/40 [==============================] - 12s 298ms/step - loss: 0.0532 - accuracy: 0.9850 - val_loss: 0.4304 - val_accuracy: 0.8287\n",
      "Epoch 13/25\n",
      "40/40 [==============================] - 11s 281ms/step - loss: 0.0386 - accuracy: 0.9927 - val_loss: 0.3574 - val_accuracy: 0.8810\n",
      "Epoch 14/25\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 0.0255 - accuracy: 0.9970 - val_loss: 0.1216 - val_accuracy: 0.9355\n",
      "Epoch 15/25\n",
      "40/40 [==============================] - 11s 263ms/step - loss: 0.0196 - accuracy: 0.9977 - val_loss: 0.0867 - val_accuracy: 0.9555\n",
      "Epoch 16/25\n",
      "40/40 [==============================] - 11s 264ms/step - loss: 0.0152 - accuracy: 0.9985 - val_loss: 0.1175 - val_accuracy: 0.9666\n",
      "Epoch 17/25\n",
      "40/40 [==============================] - 10s 262ms/step - loss: 0.0093 - accuracy: 0.9998 - val_loss: 0.0166 - val_accuracy: 0.9989\n",
      "Epoch 18/25\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 0.0071 - accuracy: 0.9998 - val_loss: 0.0097 - val_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "40/40 [==============================] - 11s 271ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.0100 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "40/40 [==============================] - 11s 267ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0033 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "40/40 [==============================] - 11s 283ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "40/40 [==============================] - 11s 266ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "40/40 [==============================] - 11s 265ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1b56d14ecc0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(generator=train_generator, \n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=test_steps,\n",
    "                    epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
